各モデルに対して、以下の手順で訓練と評価を行う。

データ分割: 各プロジェクトのデータセットを訓練データとテストデータに分割する。訓練データはモデルの学習に使用し、テストデータは最終的な性能評価に使用する。

10分割交差検証: 訓練データに対して10分割交差検証を実施し、モデルの性能の誤差を測定する。データセットを10個のサブセットに分割し、そのうち9個を訓練データ、1個を検証データとして使用する。この過程を10回繰り返し、各サブセットが1度だけ検証データとして使用されるようにする。各フェーズでF1スコア、適合率、再現率を計算し、性能のばらつきを評価する。

最終評価: 訓練されたモデルをテストデータに適用し、F1スコア、適合率、再現率、正解率、ROC-AUCを算出する。この値を各モデルの最終的な性能指標とする。

統計的有意性の検証: 提案手法（ステップ3）とベースライン（ステップ1）の予測結果に統計的に有意な差があるかを検証するため、マクネマー検定を実施する。マクネマー検定は、同じテストデータに対する2つの分類器の予測結果を比較するための統計的検定手法であり、p値が0.05未満の場合、有意な差があると判断する。

レビュー労力削減効果の評価: 提案手法によるレビュー労力削減効果を評価するため、Cost-Benefit Curveを作成する。評価手順は以下の通りである。

\begin{enumerate}
    \item 3.4節で述べた手法に従い、各コミットのレビュー労力 $W_i$ を計算する。コードチャーン、変更ファイル数、Entropyから計算したベース労力に対数変換を適用し、補正済み労力を得る。
    \item 全コミットをレビュー労力の昇順にソートし、上位80\%のコミットの労力の和を、レビューに使える総労力 $C_{total}$ として設定する。
    \item 各モデル（ベースライン、ステップ2、ステップ3）について、予測したバグ混入確率 $\hat{y}_i$ と補正済み労力 $E_{\text{adj}, i}$ から密度 $D_i = \frac{\hat{y}_i}{E_{\text{adj}, i}}$ を計算する。
    \item 密度の降順にコミットをソートし、貪欲法により累積労力が $C_{total}$ を超えない範囲でレビュー対象を選択する。各コミットをレビューするごとに、累積レビュー労力と累積発見バグ数を記録する
    \item 横軸を累積レビュー労力、縦軸を累積発見バグ数としてCost-Benefit Curveを描画する。ベースラインモデルと提案手法（ステップ2、ステップ3）のCurveを同一グラフ上に描き、視覚的に比較する
\end{enumerate}
