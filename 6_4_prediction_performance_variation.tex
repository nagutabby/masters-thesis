本研究では、5つのプロジェクトに提案手法を適用した結果、F1スコアの改善幅に最大2.6倍の差（Hazelcast: 0.11 vs Netty: 0.29）が観察された。本節では、この性能差が何に起因するかを体系的に分析する。

\paragraph{バグとコミットの特性}
ステップ1（静的コードメトリクスのみ）での予測精度と、ステップ3（変更メトリクス追加後）での改善幅の間に明確な関係が観察された。静的コードメトリクスのみを使ったモデル（ステップ1）の予測精度が比較的高いプロジェクトでは、変更メトリクス追加後のモデル（ステップ3）でもあまり予測精度が改善しない一方で、ステップ1の予測精度が比較的低いプロジェクトでは、ステップ3で予測精度が改善しやすいことが分かる。具体的には、Hazelcastはステップ1で既にF1スコア0.68と比較的高い性能を達成しており、改善の余地が限定的である。一方、Nettyはステップ1でF1スコア0.45と低い性能であったが、ステップ3で0.75まで向上し、0.29という最大の改善幅を示した。

この関係は、静的コードメトリクスと変更メトリクスの相補性を示している。静的メトリクスのみで予測が困難なプロジェクトでは、変更メトリクスが提供する時系列情報が特に有効に機能する。逆に、静的メトリクスである程度予測可能なプロジェクトでは、変更メトリクスの価値が限定的となる。

重要なのは、改善幅の違いにかかわらず、すべてのプロジェクトでステップ3の最終的なF1スコアが0.70以上に達していることである。これは、静的コードメトリクスと変更メトリクスの組み合わせにより、プロジェクト特性の違いを超えて、より安定した予測ができるようになることを示している。ステップ1のF1スコアは0.45から0.68と1.5倍の範囲でばらついていたが、ステップ3では0.70から0.79と1.1倍程度に収束している。このばらつきの減少は、提案手法が多様なプロジェクトに対して一貫した性能を提供できることを意味する。

さらに、ベースライン性能と改善幅の関係を詳しく分析するため、各プロジェクトのデータセットにおける陽性クラス（バグあり）の割合を算出し、F1スコアの改善幅との相関を調査した。表\ref{tab:imbalance_vs_improvement}に、各プロジェクトのバグあり割合とF1改善幅を示す。

\begin{table}[ht]
\centering
\caption{データセットの陽性クラス割合とF1改善幅の関係}
\label{tab:imbalance_vs_improvement}
\begin{tabular}{|l|r|r|}
\hline
プロジェクト & バグあり割合 & F1改善幅 \\
\hline
Netty & 0.22 & 0.29 \\
Neo4j & 0.26 & 0.26 \\
OrientDB & 0.26 & 0.22 \\
Elasticsearch & 0.35 & 0.19 \\
Hazelcast & 0.37 & 0.11 \\
\hline
\end{tabular}
\end{table}

この結果から、バグあり割合とF1改善幅の間に強い負の相関（Pearson相関係数 $r = -0.93$, $p = 0.020$）が観察された。すなわち、データセット中のバグを含むメソッドの割合が低いほど（データの不均衡が大きいほど）、変更メトリクスの追加による性能改善効果が大きい傾向がある。

この関係は以下のように解釈できる。データが比較的バランスの取れているプロジェクト（Hazelcast: バグあり37.5\%、Elasticsearch: 35.0\%）では、静的コードメトリクスのみでも陽性クラスと陰性クラスの判別が比較的容易であり、ベースライン性能が高い。一方、データの不均衡が大きいプロジェクト（Netty: バグあり21.8\%、Neo4j: 25.8\%）では、陽性クラスが少数派であるため静的メトリクスのみでは識別が困難であり、ベースライン性能が低くなる。このような不均衡データに対して、変更メトリクスが提供する時系列的な変化パターンは、少数派クラスを識別するための追加的な判別情報として特に有効に機能すると考えられる。

データの不均衡は機械学習における一般的な課題であるが、本研究の結果は、変更メトリクスの統合が不均衡データに対する有効な対処法の一つとなりうることを示唆している。